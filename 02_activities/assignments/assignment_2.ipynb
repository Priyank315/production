{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment, we will work with the *Adult* data set. Please download the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/2/adult). Extract the data files into the subdirectory: `../05_src/data/adult/` (relative to `./05_src/`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Assuming that the files `adult.data` and `adult.test` are in `../05_src/data/adult/`, then you can use the code below to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "    'native-country', 'income'\n",
    "]\n",
    "adult_dt = (pd.read_csv('C:/Users/prsrivastava/DSI_Demo/production/05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X and Y\n",
    "\n",
    "Create the features data frame and target data:\n",
    "\n",
    "+ Create a dataframe `X` that holds the features (all columns that are not `income`).\n",
    "+ Create a dataframe `Y` that holds the target data (`income`).\n",
    "+ From `X` and `Y`, obtain the training and testing data sets:\n",
    "\n",
    "    - Use a train-test split of 70-30%. \n",
    "    - Set the random state of the splitting function to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: X_train: (22792, 14), Y_train: (22792,)\n",
      "Testing set: X_test: (9769, 14), Y_test: (9769,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Create features (X) and target (Y)\n",
    "X = adult_dt.drop(columns=['income'])\n",
    "Y = adult_dt['income']\n",
    "\n",
    "# Split the data into training and testing sets (70-30 split, random state 42)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Output the shapes to confirm the split\n",
    "print(f\"Training set: X_train: {X_train.shape}, Y_train: {Y_train.shape}\")\n",
    "print(f\"Testing set: X_test: {X_test.shape}, Y_test: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random States\n",
    "\n",
    "Please comment: \n",
    "\n",
    "+ What is the [random state](https://scikit-learn.org/stable/glossary.html#term-random_state) of the [splitting function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)? \n",
    "+ Why is it [useful](https://en.wikipedia.org/wiki/Reproducibility)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Comment here.)*\n",
    "\n",
    "In the code, the random state is set to 42.\n",
    "The random state ensures that the data split is reproducible. When working on machine learning projects, researchers or developers may need to replicate results for debugging, comparison, or collaborative purposes. Setting a random state allows the split to be deterministic so that every time the code is run, the exact same training and testing sets are created.\n",
    "This consistency is essential for benchmarking models and ensuring reliable comparisons between different approaches or parameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Create a [Column Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) that treats the features as follows:\n",
    "\n",
    "- Numerical variables\n",
    "\n",
    "    * Apply [KNN-based imputation for completing missing values](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html):\n",
    "        \n",
    "        + Consider the 7 nearest neighbours.\n",
    "        + Weight each neighbour by the inverse of its distance, causing closer neigbours to have more influence than more distant ones.\n",
    "    * [Scale features using statistics that are robust to outliers](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler).\n",
    "\n",
    "- Categorical variables: \n",
    "    \n",
    "    * Apply a [simple imputation strategy](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer):\n",
    "\n",
    "        + Use the most frequent value to complete missing values, also called the *mode*.\n",
    "\n",
    "    * Apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html):\n",
    "        \n",
    "        + Handle unknown labels if they exist.\n",
    "        + Drop one column for binary variables.\n",
    "    \n",
    "    \n",
    "The column transformer should look like this:\n",
    "\n",
    "![](./images/assignment_2__column_transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer(transformers=[('num_transforms',\n",
      "                                 Pipeline(steps=[('KNNImputer',\n",
      "                                                  KNNImputer(n_neighbors=7,\n",
      "                                                             weights='distance')),\n",
      "                                                 ('RobustScaler',\n",
      "                                                  RobustScaler())]),\n",
      "                                 ['age', 'fnlwgt', 'education-num',\n",
      "                                  'capital-gain', 'capital-loss',\n",
      "                                  'hours-per-week']),\n",
      "                                ('cat_transforms',\n",
      "                                 Pipeline(steps=[('SimpleImputer',\n",
      "                                                  SimpleImputer(strategy='most_frequent')),\n",
      "                                                 ('OneHotEncoder',\n",
      "                                                  OneHotEncoder(drop='if_binary',\n",
      "                                                                handle_unknown='ignore'))]),\n",
      "                                 ['workclass', 'education', 'marital-status',\n",
      "                                  'occupation', 'relationship', 'race', 'sex',\n",
      "                                  'native-country'])])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline  # Import Pipeline\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_features = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship',\n",
    "                        'race', 'sex', 'native-country']\n",
    "\n",
    "# Define the transformations for numerical variables\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('KNNImputer', KNNImputer(n_neighbors=7, weights='distance')),  # KNN-based imputation\n",
    "    ('RobustScaler', RobustScaler())  # Scaling using statistics \n",
    "])\n",
    "\n",
    "# Define the transformations for categorical variables\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('SimpleImputer', SimpleImputer(strategy='most_frequent')),  # Simple imputation\n",
    "    ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore', drop='if_binary'))  # One-hot encoding with binary drop\n",
    "])\n",
    "\n",
    "# Create the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_transforms', numerical_transformer, numerical_features),  # Custom label for numerical transformations\n",
    "        ('cat_transforms', categorical_transformer, categorical_features)  # Custom label for categorical transformations\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(preprocessor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "Create a [model pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): \n",
    "\n",
    "+ Add a step labelled `preprocessing` and assign the Column Transformer from the previous section.\n",
    "+ Add a step labelled `classifier` and assign a [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to it.\n",
    "\n",
    "The pipeline looks like this:\n",
    "\n",
    "![](./images/assignment_2__pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocessing',\n",
      "                 ColumnTransformer(transformers=[('num_transforms',\n",
      "                                                  Pipeline(steps=[('KNNImputer',\n",
      "                                                                   KNNImputer(n_neighbors=7,\n",
      "                                                                              weights='distance')),\n",
      "                                                                  ('RobustScaler',\n",
      "                                                                   RobustScaler())]),\n",
      "                                                  ['age', 'fnlwgt',\n",
      "                                                   'education-num',\n",
      "                                                   'capital-gain',\n",
      "                                                   'capital-loss',\n",
      "                                                   'hours-per-week']),\n",
      "                                                 ('cat_transforms',\n",
      "                                                  Pipeline(steps=[('SimpleImputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('OneHotEncoder',\n",
      "                                                                   OneHotEncoder(drop='if_binary',\n",
      "                                                                                 handle_unknown='ignore'))]),\n",
      "                                                  ['workclass', 'education',\n",
      "                                                   'marital-status',\n",
      "                                                   'occupation', 'relationship',\n",
      "                                                   'race', 'sex',\n",
      "                                                   'native-country'])])),\n",
      "                ('classifier', RandomForestClassifier(random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the Random Forest Classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the full pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),  # Add the ColumnTransformer as a preprocessing step\n",
    "    ('classifier', classifier)  # Add RandomForestClassifier as the final step\n",
    "])\n",
    "\n",
    "# Output the pipeline structure\n",
    "print(model_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Evaluate the model pipeline using [`cross_validate()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html):\n",
    "\n",
    "+ Measure the following [preformance metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values): negative log loss, ROC AUC, accuracy, and balanced accuracy.\n",
    "+ Report the training and validation results. \n",
    "+ Use five folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prsrivastava\\.conda\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\prsrivastava\\.conda\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation Results:\n",
      "{'fit_time': array([7.93030119, 8.03047729, 7.94323897, 7.95538592, 7.92819071]), 'score_time': array([0.16977119, 0.16346836, 0.15550447, 0.15577674, 0.16130328]), 'test_neg_log_loss': array([-0.35767483, -0.36923923, -0.37598795, -0.35679093, -0.3803792 ]), 'train_neg_log_loss': array([-0.08118902, -0.08151629, -0.08146857, -0.08251057, -0.08136767]), 'test_roc_auc': array([0.90438406, 0.90107902, 0.90137791, 0.90724978, 0.9022994 ]), 'train_roc_auc': array([1., 1., 1., 1., 1.]), 'test_accuracy': array([0.85062514, 0.85040579, 0.85410268, 0.85980693, 0.85607723]), 'train_accuracy': array([0.99994515, 1.        , 0.99994516, 1.        , 1.        ]), 'test_balanced_accuracy': array([0.7744843 , 0.77188128, 0.77601679, 0.7828585 , 0.77608926]), 'train_balanced_accuracy': array([0.9998869 , 1.        , 0.99988693, 1.        , 1.        ])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, log_loss, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    'neg_log_loss': 'neg_log_loss',  # Negative log loss\n",
    "    'roc_auc': 'roc_auc',  # ROC AUC\n",
    "    'accuracy': 'accuracy',  # Accuracy\n",
    "    'balanced_accuracy': make_scorer(balanced_accuracy_score)  # Balanced accuracy\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(\n",
    "    model_pipeline,\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    cv=5,\n",
    "    scoring=scoring,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Display training and validation results\n",
    "print(\"Cross-validation Results:\")\n",
    "print(cv_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the fold-level results as a pandas data frame and sorted by negative log loss of the test (validation) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold-level Results Sorted by Validation Negative Log Loss:\n",
      "   fit_time  score_time  val_neg_log_loss  train_neg_log_loss  val_roc_auc  \\\n",
      "3  7.955386    0.155777         -0.356791           -0.082511     0.907250   \n",
      "0  7.930301    0.169771         -0.357675           -0.081189     0.904384   \n",
      "1  8.030477    0.163468         -0.369239           -0.081516     0.901079   \n",
      "2  7.943239    0.155504         -0.375988           -0.081469     0.901378   \n",
      "4  7.928191    0.161303         -0.380379           -0.081368     0.902299   \n",
      "\n",
      "   train_roc_auc  val_accuracy  train_accuracy  val_balanced_accuracy  \\\n",
      "3            1.0      0.859807        1.000000               0.782859   \n",
      "0            1.0      0.850625        0.999945               0.774484   \n",
      "1            1.0      0.850406        1.000000               0.771881   \n",
      "2            1.0      0.854103        0.999945               0.776017   \n",
      "4            1.0      0.856077        1.000000               0.776089   \n",
      "\n",
      "   train_balanced_accuracy  \n",
      "3                 1.000000  \n",
      "0                 0.999887  \n",
      "1                 1.000000  \n",
      "2                 0.999887  \n",
      "4                 1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrame\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Rename columns for clarity\n",
    "cv_df = cv_df.rename(columns={\n",
    "    'train_neg_log_loss': 'train_neg_log_loss',\n",
    "    'test_neg_log_loss': 'val_neg_log_loss',\n",
    "    'train_roc_auc': 'train_roc_auc',\n",
    "    'test_roc_auc': 'val_roc_auc',\n",
    "    'train_accuracy': 'train_accuracy',\n",
    "    'test_accuracy': 'val_accuracy',\n",
    "    'train_balanced_accuracy': 'train_balanced_accuracy',\n",
    "    'test_balanced_accuracy': 'val_balanced_accuracy',\n",
    "})\n",
    "\n",
    "# Sort by negative log loss (validation set)\n",
    "cv_df_sorted = cv_df.sort_values(by='val_neg_log_loss', ascending=False)\n",
    "print(\"\\nFold-level Results Sorted by Validation Negative Log Loss:\")\n",
    "print(cv_df_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of each metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Performance Metrics Across Folds:\n",
      "fit_time                   7.957519\n",
      "score_time                 0.161165\n",
      "val_neg_log_loss          -0.368014\n",
      "train_neg_log_loss        -0.081610\n",
      "val_roc_auc                0.903278\n",
      "train_roc_auc              1.000000\n",
      "val_accuracy               0.854204\n",
      "train_accuracy             0.999978\n",
      "val_balanced_accuracy      0.776266\n",
      "train_balanced_accuracy    0.999955\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean values of each metric\n",
    "mean_results = cv_df.mean()\n",
    "print(\"\\nMean Performance Metrics Across Folds:\")\n",
    "print(mean_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the same performance metrics (negative log loss, ROC AUC, accuracy, and balanced accuracy) using the testing data `X_test` and `Y_test`. Display results as a dictionary.\n",
    "\n",
    "*Tip*: both, `roc_auc()` and `neg_log_loss()` will require prediction scores from `pipe.predict_proba()`. However, for `roc_auc()` you should only pass the last column `Y_pred_proba[:, 1]`. Use `Y_pred_proba` with `neg_log_loss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Metrics on Testing Data:\n",
      "{'neg_log_loss': -0.39726506589398913, 'roc_auc': 0.8999828704291436, 'accuracy': 0.8551540587572934, 'balanced_accuracy': 0.7751631656838177}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# Generate predictions and prediction probabilities\n",
    "Y_pred_proba = model_pipeline.predict_proba(X_test)\n",
    "Y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate metrics on testing data\n",
    "test_metrics = {\n",
    "    'neg_log_loss': -log_loss(Y_test, Y_pred_proba),  # Negative log loss\n",
    "    'roc_auc': roc_auc_score(Y_test, Y_pred_proba[:, 1]),  # ROC AUC (using probabilities for positive class)\n",
    "    'accuracy': accuracy_score(Y_test, Y_pred),  # Accuracy\n",
    "    'balanced_accuracy': balanced_accuracy_score(Y_test, Y_pred)  # Balanced accuracy\n",
    "}\n",
    "\n",
    "# Display test metrics\n",
    "print(\"\\nPerformance Metrics on Testing Data:\")\n",
    "print(test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Recoding\n",
    "\n",
    "In the first code chunk of this document, we loaded the data and immediately recoded the target variable `income`. Why is this [convenient](https://scikit-learn.org/stable/modules/model_evaluation.html#binary-case)?\n",
    "\n",
    "The specific line was:\n",
    "\n",
    "```\n",
    "adult_dt = (pd.read_csv('../05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Answer here.)\n",
    "\n",
    "It is convenient for a streamlines workflow, making smoother compatibility with ML algorithms, reduction or manual errors by incorporation the transformatuon directly into the data loading pipleline and hence increase of the overall efficiency  during data loading. Having the target variable in a numeric format makes it easier to compute statistics, visualize data distributions, and analyze correlations with other variables.\n",
    "\n",
    "In summary, recoding income during data loading ensures a seamless, efficient, and error-free workflow for subsequent data processing and machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_2_rubric_clean.xlsx) contains the criteria for assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-2`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_2.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Becker,Barry and Kohavi,Ronny. (1996). Adult. UCI Machine Learning Repository. https://doi.org/10.24432/C5XW20."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
